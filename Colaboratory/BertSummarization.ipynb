{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertSummarization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FNP0qlRX5REq","colab_type":"code","outputId":"c853ca21-bf60-4a35-c11e-cfa260e6536e","executionInfo":{"status":"ok","timestamp":1572773109368,"user_tz":-120,"elapsed":8546,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["!pip install pytorch_pretrained_bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n","Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 32.2MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.0+cu100)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.9.11)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_pretrained_bert) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.4->boto3->pytorch_pretrained_bert) (0.15.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.4->boto3->pytorch_pretrained_bert) (1.12.0)\n","Installing collected packages: regex, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OYthVE4bWB7V","colab_type":"code","outputId":"9adc37c8-6042-47f2-91a5-ac075e983baa","executionInfo":{"status":"ok","timestamp":1572773114249,"user_tz":-120,"elapsed":13413,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["!pip install tensorboardX"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n","\r\u001b[K     |█▊                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 3.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 81kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 6.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.3)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (41.4.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-1.9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AH6mEAFxPC-V","colab_type":"text"},"source":["#### Logging"]},{"cell_type":"code","metadata":{"id":"suIXYlGHPHVO","colab_type":"code","outputId":"416ab12f-f868-4936-a6cc-f1263905665a","executionInfo":{"status":"ok","timestamp":1572773114257,"user_tz":-120,"elapsed":13413,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from __future__ import absolute_import\n","\n","import logging\n","\n","logger = logging.getLogger()\n","\n","\n","def init_logger(log_file=None, log_file_level=logging.NOTSET):\n","    log_format = logging.Formatter(\"[%(asctime)s %(levelname)s] %(message)s\")\n","    logger = logging.getLogger()\n","    logger.setLevel(logging.INFO)\n","\n","    console_handler = logging.StreamHandler()\n","    console_handler.setFormatter(log_format)\n","    logger.handlers = [console_handler]\n","\n","    if log_file and log_file != '':\n","        file_handler = logging.FileHandler(log_file)\n","        file_handler.setLevel(log_file_level)\n","        file_handler.setFormatter(log_format)\n","        logger.addHandler(file_handler)\n","\n","    return logger\n","\n","print (\"Done\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Apk6THmnOWiy","colab_type":"text"},"source":["#### Data Loader"]},{"cell_type":"code","metadata":{"id":"KrAa6ER3Od62","colab_type":"code","outputId":"7ff65df6-ca1e-4da3-8b96-01653a256a57","executionInfo":{"status":"ok","timestamp":1572773115809,"user_tz":-120,"elapsed":14949,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import gc\n","import glob\n","import random\n","\n","import torch\n","\n","class Batch(object):\n","    def _pad(self, data, pad_id, width=-1):\n","        if (width == -1):\n","            width = max(len(d) for d in data)\n","        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]\n","        return rtn_data\n","\n","    def __init__(self, data=None, device=None,  is_test=False):\n","        \"\"\"Create a Batch from a list of examples.\"\"\"\n","        if data is not None:\n","            self.batch_size = len(data)\n","            pre_src = [x[0] for x in data]\n","            pre_labels = [x[1] for x in data]\n","            pre_segs = [x[2] for x in data]\n","            pre_clss = [x[3] for x in data]\n","\n","            src = torch.tensor(self._pad(pre_src, 0))\n","\n","            labels = torch.tensor(self._pad(pre_labels, 0))\n","            segs = torch.tensor(self._pad(pre_segs, 0))\n","            #mask = 1 - (src == 0)\n","            mask = ~(src == 0)\n","\n","            clss = torch.tensor(self._pad(pre_clss, -1))\n","            #mask_cls = 1 - (clss == -1)\n","            mask_cls = ~(clss == -1)\n","            clss[clss == -1] = 0\n","\n","            setattr(self, 'clss', clss.to(device))\n","            setattr(self, 'mask_cls', mask_cls.to(device))\n","            setattr(self, 'src', src.to(device))\n","            setattr(self, 'labels', labels.to(device))\n","            setattr(self, 'segs', segs.to(device))\n","            setattr(self, 'mask', mask.to(device))\n","\n","            if (is_test):\n","                src_str = [x[-2] for x in data]\n","                setattr(self, 'src_str', src_str)\n","                tgt_str = [x[-1] for x in data]\n","                setattr(self, 'tgt_str', tgt_str)\n","\n","    def __len__(self):\n","        return self.batch_size\n","\n","\n","def batch(data, batch_size):\n","    \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n","    minibatch, size_so_far = [], 0\n","    for ex in data:\n","        minibatch.append(ex)\n","        size_so_far = simple_batch_size_fn(ex, len(minibatch))\n","        if size_so_far == batch_size:\n","            yield minibatch\n","            minibatch, size_so_far = [], 0\n","        elif size_so_far > batch_size:\n","            yield minibatch[:-1]\n","            minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n","    if minibatch:\n","        yield minibatch\n","\n","\n","def load_dataset(args, corpus_type, shuffle):\n","    \"\"\"\n","    Dataset generator. Don't do extra stuff here, like printing,\n","    because they will be postponed to the first loading time.\n","\n","    Args:\n","        corpus_type: 'train' or 'valid'\n","    Returns:\n","        A list of dataset, the dataset(s) are lazily loaded.\n","    \"\"\"\n","    assert corpus_type in [\"train\", \"valid\", \"test\"]\n","\n","    def _lazy_dataset_loader(pt_file, corpus_type):\n","        dataset = torch.load(pt_file)\n","        logger.info('Loading %s dataset from %s, number of examples: %d' %\n","                    (corpus_type, pt_file, len(dataset)))\n","        return dataset\n","\n","    # Sort the glob output by file name (by increasing indexes).\n","    pts = sorted(glob.glob(args.bert_data_path + '.' + corpus_type + '.[0-9]*.pt'))\n","    if pts:\n","        if (shuffle):\n","            random.shuffle(pts)\n","\n","        for pt in pts:\n","            yield _lazy_dataset_loader(pt, corpus_type)\n","    else:\n","        # Only one inputters.*Dataset, simple!\n","        pt = args.bert_data_path + '.' + corpus_type + '.pt'\n","        yield _lazy_dataset_loader(pt, corpus_type)\n","\n","\n","def simple_batch_size_fn(new, count):\n","    src, labels = new[0], new[1]\n","    global max_n_sents, max_n_tokens, max_size\n","    if count == 1:\n","        max_size = 0\n","        max_n_sents=0\n","        max_n_tokens=0\n","    max_n_sents = max(max_n_sents, len(src))\n","    max_size = max(max_size, max_n_sents)\n","    src_elements = count * max_size\n","    return src_elements\n","\n","\n","class Dataloader(object):\n","    def __init__(self, args, datasets,  batch_size,\n","                 device, shuffle, is_test):\n","        self.args = args\n","        self.datasets = datasets\n","        self.batch_size = batch_size\n","        self.device = device\n","        self.shuffle = shuffle\n","        self.is_test = is_test\n","        self.cur_iter = self._next_dataset_iterator(datasets)\n","\n","        assert self.cur_iter is not None\n","\n","    def __iter__(self):\n","        dataset_iter = (d for d in self.datasets)\n","        while self.cur_iter is not None:\n","            for batch in self.cur_iter:\n","                yield batch\n","            self.cur_iter = self._next_dataset_iterator(dataset_iter)\n","\n","\n","    def _next_dataset_iterator(self, dataset_iter):\n","        try:\n","            # Drop the current dataset for decreasing memory\n","            if hasattr(self, \"cur_dataset\"):\n","                self.cur_dataset = None\n","                gc.collect()\n","                del self.cur_dataset\n","                gc.collect()\n","\n","            self.cur_dataset = next(dataset_iter)\n","        except StopIteration:\n","            return None\n","\n","        return DataIterator(args = self.args,\n","            dataset=self.cur_dataset,  batch_size=self.batch_size,\n","            device=self.device, shuffle=self.shuffle, is_test=self.is_test)\n","\n","\n","class DataIterator(object):\n","    def __init__(self, args, dataset,  batch_size,  device=None, is_test=False,\n","                 shuffle=True):\n","        self.args = args\n","        self.batch_size, self.is_test, self.dataset = batch_size, is_test, dataset\n","        self.iterations = 0\n","        self.device = device\n","        self.shuffle = shuffle\n","\n","        self.sort_key = lambda x: len(x[1])\n","\n","        self._iterations_this_epoch = 0\n","\n","    def data(self):\n","        if self.shuffle:\n","            random.shuffle(self.dataset)\n","        xs = self.dataset\n","        return xs\n","\n","\n","    def preprocess(self, ex, is_test):\n","        src = ex['src']\n","        if('labels' in ex):\n","            labels = ex['labels']\n","        else:\n","            labels = ex['src_sent_labels']\n","\n","        segs = ex['segs']\n","        if(not self.args.use_interval):\n","            segs=[0]*len(segs)\n","        clss = ex['clss']\n","        src_txt = ex['src_txt']\n","        tgt_txt = ex['tgt_txt']\n","\n","        if(is_test):\n","            return src,labels,segs, clss, src_txt, tgt_txt\n","        else:\n","            return src,labels,segs, clss\n","\n","    def batch_buffer(self, data, batch_size):\n","        minibatch, size_so_far = [], 0\n","        for ex in data:\n","            if(len(ex['src'])==0):\n","                continue\n","            ex = self.preprocess(ex, self.is_test)\n","            if(ex is None):\n","                continue\n","            minibatch.append(ex)\n","            size_so_far = simple_batch_size_fn(ex, len(minibatch))\n","            if size_so_far == batch_size:\n","                yield minibatch\n","                minibatch, size_so_far = [], 0\n","            elif size_so_far > batch_size:\n","                yield minibatch[:-1]\n","                minibatch, size_so_far = minibatch[-1:], simple_batch_size_fn(ex, 1)\n","        if minibatch:\n","            yield minibatch\n","\n","    def create_batches(self):\n","        \"\"\" Create batches \"\"\"\n","        data = self.data()\n","        for buffer in self.batch_buffer(data, self.batch_size * 50):\n","\n","            p_batch = sorted(buffer, key=lambda x: len(x[3]))\n","            p_batch = batch(p_batch, self.batch_size)\n","\n","            p_batch = list(p_batch)\n","            if (self.shuffle):\n","                random.shuffle(p_batch)\n","            for b in p_batch:\n","                yield b\n","\n","    def __iter__(self):\n","        while True:\n","            self.batches = self.create_batches()\n","            for idx, minibatch in enumerate(self.batches):\n","                # fast-forward if loaded from state\n","                if self._iterations_this_epoch > idx:\n","                    continue\n","                self.iterations += 1\n","                self._iterations_this_epoch += 1\n","                batch = Batch(minibatch, self.device, self.is_test)\n","\n","                yield batch\n","            return\n","\n","print (\"Done\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FUa_IpBDRYIe","colab_type":"text"},"source":["#### Neural"]},{"cell_type":"code","metadata":{"id":"Ahyvas2gRce8","colab_type":"code","outputId":"84819fa7-a980-4ecf-dc58-4cd9383871fc","executionInfo":{"status":"ok","timestamp":1572773115811,"user_tz":-120,"elapsed":14943,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","\n","\n","def gelu(x):\n","    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    \"\"\" A two-layer Feed-Forward-Network with residual layer norm.\n","\n","    Args:\n","        d_model (int): the size of input for the first-layer of the FFN.\n","        d_ff (int): the hidden layer size of the second-layer\n","            of the FNN.\n","        dropout (float): dropout probability in :math:`[0, 1)`.\n","    \"\"\"\n","\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","        self.actv = gelu\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n","        output = self.dropout_2(self.w_2(inter))\n","        return output + x\n","\n","\n","class MultiHeadedAttention(nn.Module):\n","    \"\"\"\n","    Multi-Head Attention module from\n","    \"Attention is All You Need\"\n","    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n","\n","    Similar to standard `dot` attention but uses\n","    multiple attention distributions simulataneously\n","    to select relevant items.\n","\n","    .. mermaid::\n","\n","       graph BT\n","          A[key]\n","          B[value]\n","          C[query]\n","          O[output]\n","          subgraph Attn\n","            D[Attn 1]\n","            E[Attn 2]\n","            F[Attn N]\n","          end\n","          A --> D\n","          C --> D\n","          A --> E\n","          C --> E\n","          A --> F\n","          C --> F\n","          D --> O\n","          E --> O\n","          F --> O\n","          B --> O\n","\n","    Also includes several additional tricks.\n","\n","    Args:\n","       head_count (int): number of parallel heads\n","       model_dim (int): the dimension of keys/values/queries,\n","           must be divisible by head_count\n","       dropout (float): dropout parameter\n","    \"\"\"\n","\n","    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n","        assert model_dim % head_count == 0\n","        self.dim_per_head = model_dim // head_count\n","        self.model_dim = model_dim\n","\n","        super(MultiHeadedAttention, self).__init__()\n","        self.head_count = head_count\n","\n","        self.linear_keys = nn.Linear(model_dim,\n","                                     head_count * self.dim_per_head)\n","        self.linear_values = nn.Linear(model_dim,\n","                                       head_count * self.dim_per_head)\n","        self.linear_query = nn.Linear(model_dim,\n","                                      head_count * self.dim_per_head)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.dropout = nn.Dropout(dropout)\n","        self.use_final_linear = use_final_linear\n","        if (self.use_final_linear):\n","            self.final_linear = nn.Linear(model_dim, model_dim)\n","\n","    def forward(self, key, value, query, mask=None,\n","                layer_cache=None, type=None, predefined_graph_1=None):\n","        \"\"\"\n","        Compute the context vector and the attention vectors.\n","\n","        Args:\n","           key (`FloatTensor`): set of `key_len`\n","                key vectors `[batch, key_len, dim]`\n","           value (`FloatTensor`): set of `key_len`\n","                value vectors `[batch, key_len, dim]`\n","           query (`FloatTensor`): set of `query_len`\n","                 query vectors  `[batch, query_len, dim]`\n","           mask: binary mask indicating which keys have\n","                 non-zero attention `[batch, query_len, key_len]`\n","        Returns:\n","           (`FloatTensor`, `FloatTensor`) :\n","\n","           * output context vectors `[batch, query_len, dim]`\n","           * one of the attention vectors `[batch, query_len, key_len]`\n","        \"\"\"\n","\n","        # CHECKS\n","        # batch, k_len, d = key.size()\n","        # batch_, k_len_, d_ = value.size()\n","        # aeq(batch, batch_)\n","        # aeq(k_len, k_len_)\n","        # aeq(d, d_)\n","        # batch_, q_len, d_ = query.size()\n","        # aeq(batch, batch_)\n","        # aeq(d, d_)\n","        # aeq(self.model_dim % 8, 0)\n","        # if mask is not None:\n","        #    batch_, q_len_, k_len_ = mask.size()\n","        #    aeq(batch_, batch)\n","        #    aeq(k_len_, k_len)\n","        #    aeq(q_len_ == q_len)\n","        # END CHECKS\n","\n","        batch_size = key.size(0)\n","        dim_per_head = self.dim_per_head\n","        head_count = self.head_count\n","        key_len = key.size(1)\n","        query_len = query.size(1)\n","\n","        def shape(x):\n","            \"\"\"  projection \"\"\"\n","            return x.view(batch_size, -1, head_count, dim_per_head) \\\n","                .transpose(1, 2)\n","\n","        def unshape(x):\n","            \"\"\"  compute context \"\"\"\n","            return x.transpose(1, 2).contiguous() \\\n","                .view(batch_size, -1, head_count * dim_per_head)\n","\n","        # 1) Project key, value, and query.\n","        if layer_cache is not None:\n","            if type == \"self\":\n","                query, key, value = self.linear_query(query), \\\n","                                    self.linear_keys(query), \\\n","                                    self.linear_values(query)\n","\n","                key = shape(key)\n","                value = shape(value)\n","\n","                if layer_cache is not None:\n","                    device = key.device\n","                    if layer_cache[\"self_keys\"] is not None:\n","                        key = torch.cat(\n","                            (layer_cache[\"self_keys\"].to(device), key),\n","                            dim=2)\n","                    if layer_cache[\"self_values\"] is not None:\n","                        value = torch.cat(\n","                            (layer_cache[\"self_values\"].to(device), value),\n","                            dim=2)\n","                    layer_cache[\"self_keys\"] = key\n","                    layer_cache[\"self_values\"] = value\n","            elif type == \"context\":\n","                query = self.linear_query(query)\n","                if layer_cache is not None:\n","                    if layer_cache[\"memory_keys\"] is None:\n","                        key, value = self.linear_keys(key), \\\n","                                     self.linear_values(value)\n","                        key = shape(key)\n","                        value = shape(value)\n","                    else:\n","                        key, value = layer_cache[\"memory_keys\"], \\\n","                                     layer_cache[\"memory_values\"]\n","                    layer_cache[\"memory_keys\"] = key\n","                    layer_cache[\"memory_values\"] = value\n","                else:\n","                    key, value = self.linear_keys(key), \\\n","                                 self.linear_values(value)\n","                    key = shape(key)\n","                    value = shape(value)\n","        else:\n","            key = self.linear_keys(key)\n","            value = self.linear_values(value)\n","            query = self.linear_query(query)\n","            key = shape(key)\n","            value = shape(value)\n","\n","        query = shape(query)\n","\n","        key_len = key.size(2)\n","        query_len = query.size(2)\n","\n","        # 2) Calculate and scale scores.\n","        query = query / math.sqrt(dim_per_head)\n","        scores = torch.matmul(query, key.transpose(2, 3))\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).expand_as(scores)\n","            scores = scores.masked_fill(mask, -1e18)\n","\n","        # 3) Apply attention dropout and compute context vectors.\n","\n","        attn = self.softmax(scores)\n","\n","        if (not predefined_graph_1 is None):\n","            attn_masked = attn[:, -1] * predefined_graph_1\n","            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n","\n","            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n","\n","        drop_attn = self.dropout(attn)\n","        if (self.use_final_linear):\n","            context = unshape(torch.matmul(drop_attn, value))\n","            output = self.final_linear(context)\n","            return output\n","        else:\n","            context = torch.matmul(drop_attn, value)\n","            return context\n","\n","        # CHECK\n","        # batch_, q_len_, d_ = output.size()\n","        # aeq(q_len, q_len_)\n","        # aeq(batch, batch_)\n","        # aeq(d, d_)\n","\n","        # Return one attn\n","        \n","print (\"Done\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bqdlB2gHR1Tr","colab_type":"text"},"source":["#### RNN"]},{"cell_type":"code","metadata":{"id":"2pUL9gfPR4-l","colab_type":"code","outputId":"531814ad-08ab-4b6e-f0c9-d286eac5ea25","executionInfo":{"status":"ok","timestamp":1572773116345,"user_tz":-120,"elapsed":15470,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","\n","class LayerNormLSTMCell(nn.LSTMCell):\n","\n","    def __init__(self, input_size, hidden_size, bias=True):\n","        super().__init__(input_size, hidden_size, bias)\n","\n","        self.ln_ih = nn.LayerNorm(4 * hidden_size)\n","        self.ln_hh = nn.LayerNorm(4 * hidden_size)\n","        self.ln_ho = nn.LayerNorm(hidden_size)\n","\n","    def forward(self, input, hidden=None):\n","        self.check_forward_input(input)\n","        if hidden is None:\n","            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n","            cx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n","        else:\n","            hx, cx = hidden\n","        self.check_forward_hidden(input, hx, '[0]')\n","        self.check_forward_hidden(input, cx, '[1]')\n","\n","        gates = self.ln_ih(F.linear(input, self.weight_ih, self.bias_ih)) \\\n","                + self.ln_hh(F.linear(hx, self.weight_hh, self.bias_hh))\n","        i, f, o = gates[:, :(3 * self.hidden_size)].sigmoid().chunk(3, 1)\n","        g = gates[:, (3 * self.hidden_size):].tanh()\n","\n","        cy = (f * cx) + (i * g)\n","        hy = o * self.ln_ho(cy).tanh()\n","        return hy, cy\n","\n","\n","class LayerNormLSTM(nn.Module):\n","\n","    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, bidirectional=False):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","\n","        num_directions = 2 if bidirectional else 1\n","        self.hidden0 = nn.ModuleList([\n","            LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n","                              hidden_size=hidden_size, bias=bias)\n","            for layer in range(num_layers)\n","        ])\n","\n","        if self.bidirectional:\n","            self.hidden1 = nn.ModuleList([\n","                LayerNormLSTMCell(input_size=(input_size if layer == 0 else hidden_size * num_directions),\n","                                  hidden_size=hidden_size, bias=bias)\n","                for layer in range(num_layers)\n","            ])\n","\n","    def forward(self, input, hidden=None):\n","        seq_len, batch_size, hidden_size = input.size()  # supports TxNxH only\n","        num_directions = 2 if self.bidirectional else 1\n","        if hidden is None:\n","            hx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n","            cx = input.new_zeros(self.num_layers * num_directions, batch_size, self.hidden_size, requires_grad=False)\n","        else:\n","            hx, cx = hidden\n","\n","        ht = [[None, ] * (self.num_layers * num_directions)] * seq_len\n","        ct = [[None, ] * (self.num_layers * num_directions)] * seq_len\n","\n","        if self.bidirectional:\n","            xs = input\n","            for l, (layer0, layer1) in enumerate(zip(self.hidden0, self.hidden1)):\n","                l0, l1 = 2 * l, 2 * l + 1\n","                h0, c0, h1, c1 = hx[l0], cx[l0], hx[l1], cx[l1]\n","                for t, (x0, x1) in enumerate(zip(xs, reversed(xs))):\n","                    ht[t][l0], ct[t][l0] = layer0(x0, (h0, c0))\n","                    h0, c0 = ht[t][l0], ct[t][l0]\n","                    t = seq_len - 1 - t\n","                    ht[t][l1], ct[t][l1] = layer1(x1, (h1, c1))\n","                    h1, c1 = ht[t][l1], ct[t][l1]\n","                xs = [torch.cat((h[l0], h[l1]), dim=1) for h in ht]\n","            y = torch.stack(xs)\n","            hy = torch.stack(ht[-1])\n","            cy = torch.stack(ct[-1])\n","        else:\n","            h, c = hx, cx\n","            for t, x in enumerate(input):\n","                for l, layer in enumerate(self.hidden0):\n","                    ht[t][l], ct[t][l] = layer(x, (h[l], c[l]))\n","                    x = ht[t][l]\n","                h, c = ht[t], ct[t]\n","            y = torch.stack([h[-1] for h in ht])\n","            hy = torch.stack(ht[-1])\n","            cy = torch.stack(ct[-1])\n","\n","        return y, (hy, cy)\n","\n","print (\"Done\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iYW9bVBWQyWR","colab_type":"text"},"source":["#### Encoder"]},{"cell_type":"code","metadata":{"id":"nR2auTPUOlRA","colab_type":"code","outputId":"97c3c4b9-4af4-4e56-c815-863d9227b3d2","executionInfo":{"status":"ok","timestamp":1572773116348,"user_tz":-120,"elapsed":15466,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","\n","#from models.neural import MultiHeadedAttention, PositionwiseFeedForward\n","#from models.rnn import LayerNormLSTM\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Classifier, self).__init__()\n","        self.linear1 = nn.Linear(hidden_size, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x, mask_cls):\n","        h = self.linear1(x).squeeze(-1)\n","        sent_scores = self.sigmoid(h) * mask_cls.float()\n","        return sent_scores\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, dropout, dim, max_len=5000):\n","        pe = torch.zeros(max_len, dim)\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n","                              -(math.log(10000.0) / dim)))\n","        pe[:, 0::2] = torch.sin(position.float() * div_term)\n","        pe[:, 1::2] = torch.cos(position.float() * div_term)\n","        pe = pe.unsqueeze(0)\n","        super(PositionalEncoding, self).__init__()\n","        self.register_buffer('pe', pe)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.dim = dim\n","\n","    def forward(self, emb, step=None):\n","        emb = emb * math.sqrt(self.dim)\n","        if (step):\n","            emb = emb + self.pe[:, step][:, None, :]\n","\n","        else:\n","            emb = emb + self.pe[:, :emb.size(1)]\n","        emb = self.dropout(emb)\n","        return emb\n","\n","    def get_emb(self, emb):\n","        return self.pe[:, :emb.size(1)]\n","\n","\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads, d_ff, dropout):\n","        super(TransformerEncoderLayer, self).__init__()\n","\n","        self.self_attn = MultiHeadedAttention(\n","            heads, d_model, dropout=dropout)\n","        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, iter, query, inputs, mask):\n","        if (iter != 0):\n","            input_norm = self.layer_norm(inputs)\n","        else:\n","            input_norm = inputs\n","\n","        mask = mask.unsqueeze(1)\n","        context = self.self_attn(input_norm, input_norm, input_norm,\n","                                 mask=mask)\n","        out = self.dropout(context) + inputs\n","        return self.feed_forward(out)\n","\n","\n","class TransformerInterEncoder(nn.Module):\n","    def __init__(self, d_model, d_ff, heads, dropout, num_inter_layers=0):\n","        super(TransformerInterEncoder, self).__init__()\n","        self.d_model = d_model\n","        self.num_inter_layers = num_inter_layers\n","        self.pos_emb = PositionalEncoding(dropout, d_model)\n","        self.transformer_inter = nn.ModuleList(\n","            [TransformerEncoderLayer(d_model, heads, d_ff, dropout)\n","             for _ in range(num_inter_layers)])\n","        self.dropout = nn.Dropout(dropout)\n","        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n","        self.wo = nn.Linear(d_model, 1, bias=True)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, top_vecs, mask):\n","        \"\"\" See :obj:`EncoderBase.forward()`\"\"\"\n","\n","        batch_size, n_sents = top_vecs.size(0), top_vecs.size(1)\n","        pos_emb = self.pos_emb.pe[:, :n_sents]\n","        x = top_vecs * mask[:, :, None].float()\n","        x = x + pos_emb\n","\n","        for i in range(self.num_inter_layers):\n","            #x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_sents * max_tokens * dim\n","            x = self.transformer_inter[i](i, x, x, ~mask)\n","\n","        x = self.layer_norm(x)\n","        sent_scores = self.sigmoid(self.wo(x))\n","        sent_scores = sent_scores.squeeze(-1) * mask.float()\n","\n","        return sent_scores\n","\n","\n","class RNNEncoder(nn.Module):\n","\n","    def __init__(self, bidirectional, num_layers, input_size,\n","                 hidden_size, dropout=0.0):\n","        super(RNNEncoder, self).__init__()\n","        num_directions = 2 if bidirectional else 1\n","        assert hidden_size % num_directions == 0\n","        hidden_size = hidden_size // num_directions\n","\n","        self.rnn = LayerNormLSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            bidirectional=bidirectional)\n","\n","        self.wo = nn.Linear(num_directions * hidden_size, 1, bias=True)\n","        self.dropout = nn.Dropout(dropout)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x, mask):\n","        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n","        x = torch.transpose(x, 1, 0)\n","        memory_bank, _ = self.rnn(x)\n","        memory_bank = self.dropout(memory_bank) + x\n","        memory_bank = torch.transpose(memory_bank, 1, 0)\n","\n","        sent_scores = self.sigmoid(self.wo(memory_bank))\n","        sent_scores = sent_scores.squeeze(-1) * mask.float()\n","        return sent_scores\n","\n","print (\"Done\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yEg8_IV4TWNG","colab_type":"text"},"source":["#### Optimizers"]},{"cell_type":"code","metadata":{"id":"c-tU_fXOTckE","colab_type":"code","outputId":"0f78bcea-9fdd-4529-82ed-ac32d860b14d","executionInfo":{"status":"ok","timestamp":1572773116819,"user_tz":-120,"elapsed":15929,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import torch\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_norm_\n","\n","\n","# from onmt.utils import use_gpu\n","\n","\n","def use_gpu(opt):\n","    \"\"\"\n","    Creates a boolean if gpu used\n","    \"\"\"\n","    return (hasattr(opt, 'gpu_ranks') and len(opt.gpu_ranks) > 0) or \\\n","           (hasattr(opt, 'gpu') and opt.gpu > -1)\n","\n","def build_optim(model, opt, checkpoint):\n","    \"\"\" Build optimizer \"\"\"\n","    saved_optimizer_state_dict = None\n","\n","    if opt.train_from:\n","        optim = checkpoint['optim']\n","        # We need to save a copy of optim.optimizer.state_dict() for setting\n","        # the, optimizer state later on in Stage 2 in this method, since\n","        # the method optim.set_parameters(model.parameters()) will overwrite\n","        # optim.optimizer, and with ith the values stored in\n","        # optim.optimizer.state_dict()\n","        saved_optimizer_state_dict = optim.optimizer.state_dict()\n","    else:\n","        optim = Optimizer(\n","            opt.optim, opt.learning_rate, opt.max_grad_norm,\n","            lr_decay=opt.learning_rate_decay,\n","            start_decay_steps=opt.start_decay_steps,\n","            decay_steps=opt.decay_steps,\n","            beta1=opt.adam_beta1,\n","            beta2=opt.adam_beta2,\n","            adagrad_accum=opt.adagrad_accumulator_init,\n","            decay_method=opt.decay_method,\n","            warmup_steps=opt.warmup_steps)\n","\n","    # Stage 1:\n","    # Essentially optim.set_parameters (re-)creates and optimizer using\n","    # model.paramters() as parameters that will be stored in the\n","    # optim.optimizer.param_groups field of the torch optimizer class.\n","    # Importantly, this method does not yet load the optimizer state, as\n","    # essentially it builds a new optimizer with empty optimizer state and\n","    # parameters from the model.\n","    optim.set_parameters(model.named_parameters())\n","\n","    if opt.train_from:\n","        # Stage 2: In this stage, which is only performed when loading an\n","        # optimizer from a checkpoint, we load the saved_optimizer_state_dict\n","        # into the re-created optimizer, to set the optim.optimizer.state\n","        # field, which was previously empty. For this, we use the optimizer\n","        # state saved in the \"saved_optimizer_state_dict\" variable for\n","        # this purpose.\n","        # See also: https://github.com/pytorch/pytorch/issues/2830\n","        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n","        # Convert back the state values to cuda type if applicable\n","        if use_gpu(opt):\n","            for state in optim.optimizer.state.values():\n","                for k, v in state.items():\n","                    if torch.is_tensor(v):\n","                        state[k] = v.cuda()\n","\n","        # We want to make sure that indeed we have a non-empty optimizer state\n","        # when we loaded an existing model. This should be at least the case\n","        # for Adam, which saves \"exp_avg\" and \"exp_avg_sq\" state\n","        # (Exponential moving average of gradient and squared gradient values)\n","        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n","            raise RuntimeError(\n","                \"Error: loaded Adam optimizer from existing model\" +\n","                \" but optimizer state is empty\")\n","\n","    return optim\n","\n","\n","class MultipleOptimizer(object):\n","    \"\"\" Implement multiple optimizers needed for sparse adam \"\"\"\n","\n","    def __init__(self, op):\n","        \"\"\" ? \"\"\"\n","        self.optimizers = op\n","\n","    def zero_grad(self):\n","        \"\"\" ? \"\"\"\n","        for op in self.optimizers:\n","            op.zero_grad()\n","\n","    def step(self):\n","        \"\"\" ? \"\"\"\n","        for op in self.optimizers:\n","            op.step()\n","\n","    @property\n","    def state(self):\n","        \"\"\" ? \"\"\"\n","        return {k: v for op in self.optimizers for k, v in op.state.items()}\n","\n","    def state_dict(self):\n","        \"\"\" ? \"\"\"\n","        return [op.state_dict() for op in self.optimizers]\n","\n","    def load_state_dict(self, state_dicts):\n","        \"\"\" ? \"\"\"\n","        assert len(state_dicts) == len(self.optimizers)\n","        for i in range(len(state_dicts)):\n","            self.optimizers[i].load_state_dict(state_dicts[i])\n","\n","\n","class Optimizer(object):\n","    \"\"\"\n","    Controller class for optimization. Mostly a thin\n","    wrapper for `optim`, but also useful for implementing\n","    rate scheduling beyond what is currently available.\n","    Also implements necessary methods for training RNNs such\n","    as grad manipulations.\n","\n","    Args:\n","      method (:obj:`str`): one of [sgd, adagrad, adadelta, adam]\n","      lr (float): learning rate\n","      lr_decay (float, optional): learning rate decay multiplier\n","      start_decay_steps (int, optional): step to start learning rate decay\n","      beta1, beta2 (float, optional): parameters for adam\n","      adagrad_accum (float, optional): initialization parameter for adagrad\n","      decay_method (str, option): custom decay options\n","      warmup_steps (int, option): parameter for `noam` decay\n","\n","    We use the default parameters for Adam that are suggested by\n","    the original paper https://arxiv.org/pdf/1412.6980.pdf\n","    These values are also used by other established implementations,\n","    e.g. https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n","    https://keras.io/optimizers/\n","    Recently there are slightly different values used in the paper\n","    \"Attention is all you need\"\n","    https://arxiv.org/pdf/1706.03762.pdf, particularly the value beta2=0.98\n","    was used there however, beta2=0.999 is still arguably the more\n","    established value, so we use that here as well\n","    \"\"\"\n","\n","    def __init__(self, method, learning_rate, max_grad_norm,\n","                 lr_decay=1, start_decay_steps=None, decay_steps=None,\n","                 beta1=0.9, beta2=0.999,\n","                 adagrad_accum=0.0,\n","                 decay_method=None,\n","                 warmup_steps=4000\n","                 ):\n","        self.last_ppl = None\n","        self.learning_rate = learning_rate\n","        self.original_lr = learning_rate\n","        self.max_grad_norm = max_grad_norm\n","        self.method = method\n","        self.lr_decay = lr_decay\n","        self.start_decay_steps = start_decay_steps\n","        self.decay_steps = decay_steps\n","        self.start_decay = False\n","        self._step = 0\n","        self.betas = [beta1, beta2]\n","        self.adagrad_accum = adagrad_accum\n","        self.decay_method = decay_method\n","        self.warmup_steps = warmup_steps\n","\n","    def set_parameters(self, params):\n","        \"\"\" ? \"\"\"\n","        self.params = []\n","        self.sparse_params = []\n","        for k, p in params:\n","            if p.requires_grad:\n","                if self.method != 'sparseadam' or \"embed\" not in k:\n","                    self.params.append(p)\n","                else:\n","                    self.sparse_params.append(p)\n","        if self.method == 'sgd':\n","            self.optimizer = optim.SGD(self.params, lr=self.learning_rate)\n","        elif self.method == 'adagrad':\n","            self.optimizer = optim.Adagrad(self.params, lr=self.learning_rate)\n","            for group in self.optimizer.param_groups:\n","                for p in group['params']:\n","                    self.optimizer.state[p]['sum'] = self.optimizer\\\n","                        .state[p]['sum'].fill_(self.adagrad_accum)\n","        elif self.method == 'adadelta':\n","            self.optimizer = optim.Adadelta(self.params, lr=self.learning_rate)\n","        elif self.method == 'adam':\n","            self.optimizer = optim.Adam(self.params, lr=self.learning_rate,\n","                                        betas=self.betas, eps=1e-9)\n","        elif self.method == 'sparseadam':\n","            self.optimizer = MultipleOptimizer(\n","                [optim.Adam(self.params, lr=self.learning_rate,\n","                            betas=self.betas, eps=1e-8),\n","                 optim.SparseAdam(self.sparse_params, lr=self.learning_rate,\n","                                  betas=self.betas, eps=1e-8)])\n","        else:\n","            raise RuntimeError(\"Invalid optim method: \" + self.method)\n","\n","    def _set_rate(self, learning_rate):\n","        self.learning_rate = learning_rate\n","        if self.method != 'sparseadam':\n","            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n","        else:\n","            for op in self.optimizer.optimizers:\n","                op.param_groups[0]['lr'] = self.learning_rate\n","\n","    def step(self):\n","        \"\"\"Update the model parameters based on current gradients.\n","\n","        Optionally, will employ gradient modification or update learning\n","        rate.\n","        \"\"\"\n","        self._step += 1\n","\n","        # Decay method used in tensor2tensor.\n","        if self.decay_method == \"noam\":\n","            self._set_rate(\n","                self.original_lr *\n","\n","                 min(self._step ** (-0.5),\n","                     self._step * self.warmup_steps**(-1.5)))\n","\n","            # self._set_rate(self.original_lr *self.model_size ** (-0.5) *min(1.0, self._step / self.warmup_steps)*max(self._step, self.warmup_steps)**(-0.5))\n","        # Decay based on start_decay_steps every decay_steps\n","        else:\n","            if ((self.start_decay_steps is not None) and (\n","                     self._step >= self.start_decay_steps)):\n","                self.start_decay = True\n","            if self.start_decay:\n","                if ((self._step - self.start_decay_steps)\n","                   % self.decay_steps == 0):\n","                    self.learning_rate = self.learning_rate * self.lr_decay\n","\n","        if self.method != 'sparseadam':\n","            self.optimizer.param_groups[0]['lr'] = self.learning_rate\n","\n","        if self.max_grad_norm:\n","            clip_grad_norm_(self.params, self.max_grad_norm)\n","        self.optimizer.step()\n","\n","print (\"Done\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l3X0omTASxwO","colab_type":"text"},"source":["#### Model Builder"]},{"cell_type":"code","metadata":{"id":"zFYz3RFEQ7f2","colab_type":"code","outputId":"d85083de-ceb5-4e9a-ba20-8664423a6055","executionInfo":{"status":"ok","timestamp":1572773116821,"user_tz":-120,"elapsed":15924,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import torch\n","import torch.nn as nn\n","from pytorch_pretrained_bert import BertModel, BertConfig\n","from torch.nn.init import xavier_uniform_\n","\n","#from models.encoder import TransformerInterEncoder, Classifier, RNNEncoder\n","#from models.optimizers import Optimizer\n","\n","\n","def build_optim(args, model, checkpoint):\n","    \"\"\" Build optimizer \"\"\"\n","    saved_optimizer_state_dict = None\n","\n","    if args.train_from != '':\n","        optim = checkpoint['optim']\n","        saved_optimizer_state_dict = optim.optimizer.state_dict()\n","    else:\n","        optim = Optimizer(\n","            args.optim, args.lr, args.max_grad_norm,\n","            beta1=args.beta1, beta2=args.beta2,\n","            decay_method=args.decay_method,\n","            warmup_steps=args.warmup_steps)\n","\n","    optim.set_parameters(list(model.named_parameters()))\n","\n","    if args.train_from != '':\n","        optim.optimizer.load_state_dict(saved_optimizer_state_dict)\n","        if args.visible_gpus != '-1':\n","            for state in optim.optimizer.state.values():\n","                for k, v in state.items():\n","                    if torch.is_tensor(v):\n","                        state[k] = v.cuda()\n","\n","        if (optim.method == 'adam') and (len(optim.optimizer.state) < 1):\n","            raise RuntimeError(\n","                \"Error: loaded Adam optimizer from existing model\" +\n","                \" but optimizer state is empty\")\n","\n","    return optim\n","\n","\n","class Bert(nn.Module):\n","    def __init__(self, temp_dir, load_pretrained_bert, bert_config):\n","        super(Bert, self).__init__()\n","        if(load_pretrained_bert):\n","            self.model = BertModel.from_pretrained('bert-base-multilingual-cased', cache_dir=temp_dir)\n","        else:\n","            self.model = BertModel(bert_config)\n","\n","    def forward(self, x, segs, mask):\n","        encoded_layers, _ = self.model(x, segs, attention_mask =mask)\n","        top_vec = encoded_layers[-1]\n","        return top_vec\n","\n","\n","\n","class Summarizer(nn.Module):\n","    def __init__(self, args, device, load_pretrained_bert = False, bert_config = None):\n","        super(Summarizer, self).__init__()\n","        self.args = args\n","        self.device = device\n","        self.bert = Bert(args.temp_dir, load_pretrained_bert, bert_config)\n","        if (args.encoder == 'classifier'):\n","            self.encoder = Classifier(self.bert.model.config.hidden_size)\n","        elif(args.encoder=='transformer'):\n","            self.encoder = TransformerInterEncoder(self.bert.model.config.hidden_size, args.ff_size, args.heads,\n","                                                   args.dropout, args.inter_layers)\n","        elif(args.encoder=='rnn'):\n","            self.encoder = RNNEncoder(bidirectional=True, num_layers=1,\n","                                      input_size=self.bert.model.config.hidden_size, hidden_size=args.rnn_size,\n","                                      dropout=args.dropout)\n","        elif (args.encoder == 'baseline'):\n","            bert_config = BertConfig(self.bert.model.config.vocab_size, hidden_size=args.hidden_size,\n","                                     num_hidden_layers=6, num_attention_heads=8, intermediate_size=args.ff_size)\n","            self.bert.model = BertModel(bert_config)\n","            self.encoder = Classifier(self.bert.model.config.hidden_size)\n","\n","        if args.param_init != 0.0:\n","            for p in self.encoder.parameters():\n","                p.data.uniform_(-args.param_init, args.param_init)\n","        if args.param_init_glorot:\n","            for p in self.encoder.parameters():\n","                if p.dim() > 1:\n","                    xavier_uniform_(p)\n","\n","        self.to(device)\n","    def load_cp(self, pt):\n","        self.load_state_dict(pt['model'], strict=True)\n","\n","    def forward(self, x, segs, clss, mask, mask_cls, sentence_range=None):\n","\n","        top_vec = self.bert(x, segs, mask)\n","        sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), clss]\n","        sents_vec = sents_vec * mask_cls[:, :, None].float()\n","        sent_scores = self.encoder(sents_vec, mask_cls).squeeze(-1)\n","        return sent_scores, mask_cls\n","\n","print (\"Done\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pBfcbWG_nyPZ","colab_type":"text"},"source":["#### Distributed"]},{"cell_type":"code","metadata":{"id":"QIkU0dU5n3Nv","colab_type":"code","colab":{}},"source":["\"\"\" Pytorch Distributed utils\n","    This piece of code was heavily inspired by the equivalent of Fairseq-py\n","    https://github.com/pytorch/fairseq\n","\"\"\"\n","\n","\n","from __future__ import print_function\n","\n","import math\n","import pickle\n","\n","import torch.distributed\n","\n","\n","def is_master(gpu_ranks, device_id):\n","    return gpu_ranks[device_id] == 0\n","\n","\n","def multi_init(device_id, world_size,gpu_ranks):\n","    print(gpu_ranks)\n","    dist_init_method = 'tcp://localhost:10000'\n","    dist_world_size = world_size\n","    torch.distributed.init_process_group(\n","        backend='nccl', init_method=dist_init_method,\n","        world_size=dist_world_size, rank=gpu_ranks[device_id])\n","    gpu_rank = torch.distributed.get_rank()\n","    if not is_master(gpu_ranks, device_id):\n","    #     print('not master')\n","        logger.disabled = True\n","\n","    return gpu_rank\n","\n","\n","\n","def all_reduce_and_rescale_tensors(tensors, rescale_denom,\n","                                   buffer_size=10485760):\n","    \"\"\"All-reduce and rescale tensors in chunks of the specified size.\n","\n","    Args:\n","        tensors: list of Tensors to all-reduce\n","        rescale_denom: denominator for rescaling summed Tensors\n","        buffer_size: all-reduce chunk size in bytes\n","    \"\"\"\n","    # buffer size in bytes, determine equiv. # of elements based on data type\n","    buffer_t = tensors[0].new(\n","        math.ceil(buffer_size / tensors[0].element_size())).zero_()\n","    buffer = []\n","\n","    def all_reduce_buffer():\n","        # copy tensors into buffer_t\n","        offset = 0\n","        for t in buffer:\n","            numel = t.numel()\n","            buffer_t[offset:offset+numel].copy_(t.view(-1))\n","            offset += numel\n","\n","        # all-reduce and rescale\n","        torch.distributed.all_reduce(buffer_t[:offset])\n","        buffer_t.div_(rescale_denom)\n","\n","        # copy all-reduced buffer back into tensors\n","        offset = 0\n","        for t in buffer:\n","            numel = t.numel()\n","            t.view(-1).copy_(buffer_t[offset:offset+numel])\n","            offset += numel\n","\n","    filled = 0\n","    for t in tensors:\n","        sz = t.numel() * t.element_size()\n","        if sz > buffer_size:\n","            # tensor is bigger than buffer, all-reduce and rescale directly\n","            torch.distributed.all_reduce(t)\n","            t.div_(rescale_denom)\n","        elif filled + sz > buffer_size:\n","            # buffer is full, all-reduce and replace buffer with grad\n","            all_reduce_buffer()\n","            buffer = [t]\n","            filled = sz\n","        else:\n","            # add tensor to buffer\n","            buffer.append(t)\n","            filled += sz\n","\n","    if len(buffer) > 0:\n","        all_reduce_buffer()\n","\n","\n","def all_gather_list(data, max_size=4096):\n","    \"\"\"Gathers arbitrary data from all nodes into a list.\"\"\"\n","    world_size = torch.distributed.get_world_size()\n","    if not hasattr(all_gather_list, '_in_buffer') or \\\n","            max_size != all_gather_list._in_buffer.size():\n","        all_gather_list._in_buffer = torch.cuda.ByteTensor(max_size)\n","        all_gather_list._out_buffers = [\n","            torch.cuda.ByteTensor(max_size)\n","            for i in range(world_size)\n","        ]\n","    in_buffer = all_gather_list._in_buffer\n","    out_buffers = all_gather_list._out_buffers\n","\n","    enc = pickle.dumps(data)\n","    enc_size = len(enc)\n","    if enc_size + 2 > max_size:\n","        raise ValueError(\n","            'encoded data exceeds max_size: {}'.format(enc_size + 2))\n","    assert max_size < 255*256\n","    in_buffer[0] = enc_size // 255  # this encoding works for max_size < 65k\n","    in_buffer[1] = enc_size % 255\n","    in_buffer[2:enc_size+2] = torch.ByteTensor(list(enc))\n","\n","    torch.distributed.all_gather(out_buffers, in_buffer.cuda())\n","\n","    results = []\n","    for i in range(world_size):\n","        out_buffer = out_buffers[i]\n","        size = (255 * out_buffer[0].item()) + out_buffer[1].item()\n","\n","        bytes_list = bytes(out_buffer[2:size+2].tolist())\n","        result = pickle.loads(bytes_list)\n","        results.append(result)\n","    return results\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ZJzOClZUsRL","colab_type":"text"},"source":["#### Stats"]},{"cell_type":"code","metadata":{"id":"8Y64cAsnUwFi","colab_type":"code","outputId":"b2a96d79-8e68-42db-9374-abec2475dc70","executionInfo":{"status":"ok","timestamp":1572773116825,"user_tz":-120,"elapsed":15914,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from __future__ import division\n","\n","import sys\n","import time\n","\n","#from others.logging import logger\n","\n","\n","class Statistics(object):\n","    \"\"\"\n","    Accumulator for loss statistics.\n","    Currently calculates:\n","\n","    * accuracy\n","    * perplexity\n","    * elapsed time\n","    \"\"\"\n","\n","    def __init__(self, loss=0, n_docs=0, n_correct=0):\n","        self.loss = loss\n","        self.n_docs = n_docs\n","        self.start_time = time.time()\n","\n","    @staticmethod\n","    def all_gather_stats(stat, max_size=4096):\n","        \"\"\"\n","        Gather a `Statistics` object accross multiple process/nodes\n","\n","        Args:\n","            stat(:obj:Statistics): the statistics object to gather\n","                accross all processes/nodes\n","            max_size(int): max buffer size to use\n","\n","        Returns:\n","            `Statistics`, the update stats object\n","        \"\"\"\n","        stats = Statistics.all_gather_stats_list([stat], max_size=max_size)\n","        return stats[0]\n","\n","    @staticmethod\n","    def all_gather_stats_list(stat_list, max_size=4096):\n","        \"\"\"\n","        Gather a `Statistics` list accross all processes/nodes\n","\n","        Args:\n","            stat_list(list([`Statistics`])): list of statistics objects to\n","                gather accross all processes/nodes\n","            max_size(int): max buffer size to use\n","\n","        Returns:\n","            our_stats(list([`Statistics`])): list of updated stats\n","        \"\"\"\n","        from torch.distributed import get_rank\n","        #from distributed import all_gather_list\n","\n","        # Get a list of world_size lists with len(stat_list) Statistics objects\n","        all_stats = all_gather_list(stat_list, max_size=max_size)\n","\n","        our_rank = get_rank()\n","        our_stats = all_stats[our_rank]\n","        for other_rank, stats in enumerate(all_stats):\n","            if other_rank == our_rank:\n","                continue\n","            for i, stat in enumerate(stats):\n","                our_stats[i].update(stat, update_n_src_words=True)\n","        return our_stats\n","\n","    def update(self, stat, update_n_src_words=False):\n","        \"\"\"\n","        Update statistics by suming values with another `Statistics` object\n","\n","        Args:\n","            stat: another statistic object\n","            update_n_src_words(bool): whether to update (sum) `n_src_words`\n","                or not\n","\n","        \"\"\"\n","        self.loss += stat.loss\n","\n","        self.n_docs += stat.n_docs\n","\n","    def xent(self):\n","        \"\"\" compute cross entropy \"\"\"\n","        if(self.n_docs==0):\n","            return 0\n","        return self.loss/self.n_docs\n","\n","\n","    def elapsed_time(self):\n","        \"\"\" compute elapsed time \"\"\"\n","        return time.time() - self.start_time\n","\n","    def output(self, step, num_steps, learning_rate, start):\n","        \"\"\"Write out statistics to stdout.\n","\n","        Args:\n","           step (int): current step\n","           n_batch (int): total batches\n","           start (int): start time of step.\n","        \"\"\"\n","        t = self.elapsed_time()\n","        step_fmt = \"%2d\" % step\n","        if num_steps > 0:\n","            step_fmt = \"%s/%5d\" % (step_fmt, num_steps)\n","        logger.info(\n","            (\"Step %s; xent: %4.2f; \" +\n","             \"lr: %7.7f; %3.0f docs/s; %6.0f sec\")\n","            % (step_fmt,\n","               self.xent(),\n","               learning_rate,\n","               self.n_docs / (t + 1e-5),\n","               time.time() - start))\n","        sys.stdout.flush()\n","\n","    def log_tensorboard(self, prefix, writer, learning_rate, step):\n","        \"\"\" display statistics to tensorboard \"\"\"\n","        t = self.elapsed_time()\n","        writer.add_scalar(prefix + \"/xent\", self.xent(), step)\n","        writer.add_scalar(prefix + \"/lr\", learning_rate, step)\n","\n","print (\"Done\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WVpsqBaqUPNO","colab_type":"text"},"source":["#### Reporter"]},{"cell_type":"code","metadata":{"id":"OmnlYFl6S-ZA","colab_type":"code","outputId":"17e31fdf-0180-4bb6-90e3-8ca009f25692","executionInfo":{"status":"ok","timestamp":1572773116828,"user_tz":-120,"elapsed":15910,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from __future__ import print_function\n","\n","import time\n","from datetime import datetime\n","\n","#from models.stats import Statistics\n","#from others.logging import logger\n","\n","\n","def build_report_manager(opt):\n","    if opt.tensorboard:\n","        from tensorboardX import SummaryWriter\n","        tensorboard_log_dir = opt.tensorboard_log_dir\n","\n","        if not opt.train_from:\n","            tensorboard_log_dir += datetime.now().strftime(\"/%b-%d_%H-%M-%S\")\n","\n","        writer = SummaryWriter(tensorboard_log_dir,\n","                               comment=\"Unmt\")\n","    else:\n","        writer = None\n","\n","    report_mgr = ReportMgr(opt.report_every, start_time=-1,\n","                           tensorboard_writer=writer)\n","    return report_mgr\n","\n","\n","class ReportMgrBase(object):\n","    \"\"\"\n","    Report Manager Base class\n","    Inherited classes should override:\n","        * `_report_training`\n","        * `_report_step`\n","    \"\"\"\n","\n","    def __init__(self, report_every, start_time=-1.):\n","        \"\"\"\n","        Args:\n","            report_every(int): Report status every this many sentences\n","            start_time(float): manually set report start time. Negative values\n","                means that you will need to set it later or use `start()`\n","        \"\"\"\n","        self.report_every = report_every\n","        self.progress_step = 0\n","        self.start_time = start_time\n","\n","    def start(self):\n","        self.start_time = time.time()\n","\n","    def log(self, *args, **kwargs):\n","        logger.info(*args, **kwargs)\n","\n","    def report_training(self, step, num_steps, learning_rate,\n","                        report_stats, multigpu=False):\n","        \"\"\"\n","        This is the user-defined batch-level traing progress\n","        report function.\n","\n","        Args:\n","            step(int): current step count.\n","            num_steps(int): total number of batches.\n","            learning_rate(float): current learning rate.\n","            report_stats(Statistics): old Statistics instance.\n","        Returns:\n","            report_stats(Statistics): updated Statistics instance.\n","        \"\"\"\n","        if self.start_time < 0:\n","            raise ValueError(\"\"\"ReportMgr needs to be started\n","                                (set 'start_time' or use 'start()'\"\"\")\n","\n","        if step % self.report_every == 0:\n","            if multigpu:\n","                report_stats = \\\n","                    Statistics.all_gather_stats(report_stats)\n","            self._report_training(\n","                step, num_steps, learning_rate, report_stats)\n","            self.progress_step += 1\n","            return Statistics()\n","        else:\n","            return report_stats\n","\n","    def _report_training(self, *args, **kwargs):\n","        \"\"\" To be overridden \"\"\"\n","        raise NotImplementedError()\n","\n","    def report_step(self, lr, step, train_stats=None, valid_stats=None):\n","        \"\"\"\n","        Report stats of a step\n","\n","        Args:\n","            train_stats(Statistics): training stats\n","            valid_stats(Statistics): validation stats\n","            lr(float): current learning rate\n","        \"\"\"\n","        self._report_step(\n","            lr, step, train_stats=train_stats, valid_stats=valid_stats)\n","\n","    def _report_step(self, *args, **kwargs):\n","        raise NotImplementedError()\n","\n","\n","class ReportMgr(ReportMgrBase):\n","    def __init__(self, report_every, start_time=-1., tensorboard_writer=None):\n","        \"\"\"\n","        A report manager that writes statistics on standard output as well as\n","        (optionally) TensorBoard\n","\n","        Args:\n","            report_every(int): Report status every this many sentences\n","            tensorboard_writer(:obj:`tensorboard.SummaryWriter`):\n","                The TensorBoard Summary writer to use or None\n","        \"\"\"\n","        super(ReportMgr, self).__init__(report_every, start_time)\n","        self.tensorboard_writer = tensorboard_writer\n","\n","    def maybe_log_tensorboard(self, stats, prefix, learning_rate, step):\n","        if self.tensorboard_writer is not None:\n","            stats.log_tensorboard(\n","                prefix, self.tensorboard_writer, learning_rate, step)\n","\n","    def _report_training(self, step, num_steps, learning_rate,\n","                         report_stats):\n","        \"\"\"\n","        See base class method `ReportMgrBase.report_training`.\n","        \"\"\"\n","        report_stats.output(step, num_steps,\n","                            learning_rate, self.start_time)\n","\n","        # Log the progress using the number of batches on the x-axis.\n","        self.maybe_log_tensorboard(report_stats,\n","                                   \"progress\",\n","                                   learning_rate,\n","                                   self.progress_step)\n","        report_stats = Statistics()\n","\n","        return report_stats\n","\n","    def _report_step(self, lr, step, train_stats=None, valid_stats=None):\n","        \"\"\"\n","        See base class method `ReportMgrBase.report_step`.\n","        \"\"\"\n","        if train_stats is not None:\n","            self.log('Train xent: %g' % train_stats.xent())\n","\n","            self.maybe_log_tensorboard(train_stats,\n","                                       \"train\",\n","                                       lr,\n","                                       step)\n","\n","        if valid_stats is not None:\n","            self.log('Validation xent: %g at step %d' % (valid_stats.xent(), step))\n","\n","            self.maybe_log_tensorboard(valid_stats,\n","                                       \"valid\",\n","                                       lr,\n","                                       step)\n","\n","print (\"Done\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jt8FjJV7VPMI","colab_type":"text"},"source":["#### Trainer"]},{"cell_type":"code","metadata":{"id":"Ef7MVVXMUdSC","colab_type":"code","outputId":"7abff559-6c7f-419e-9ffc-01b4a03acf7e","executionInfo":{"status":"ok","timestamp":1572773118781,"user_tz":-120,"elapsed":17856,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","\n","import numpy as np\n","import torch\n","from tensorboardX import SummaryWriter\n","\n","#import distributed\n","# import onmt\n","#from models.reporter import ReportMgr\n","#from models.stats import Statistics\n","#from others.logging import logger\n","#from others.utils import test_rouge, rouge_results_to_str\n","\n","\n","def _tally_parameters(model):\n","    n_params = sum([p.nelement() for p in model.parameters()])\n","    return n_params\n","\n","\n","def build_trainer(args, device_id, model,\n","                  optim):\n","    \"\"\"\n","    Simplify `Trainer` creation based on user `opt`s*\n","    Args:\n","        opt (:obj:`Namespace`): user options (usually from argument parsing)\n","        model (:obj:`onmt.models.NMTModel`): the model to train\n","        fields (dict): dict of fields\n","        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n","        data_type (str): string describing the type of data\n","            e.g. \"text\", \"img\", \"audio\"\n","        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n","            used to save the model\n","    \"\"\"\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","\n","\n","    grad_accum_count = args.accum_count\n","    n_gpu = args.world_size\n","\n","    if device_id >= 0:\n","        gpu_rank = int(args.gpu_ranks[device_id])\n","    else:\n","        gpu_rank = 0\n","        n_gpu = 0\n","\n","    print('gpu_rank %d' % gpu_rank)\n","\n","    tensorboard_log_dir = args.model_path\n","\n","    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n","\n","    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n","\n","    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n","\n","    # print(tr)\n","    if (model):\n","        n_params = _tally_parameters(model)\n","        logger.info('* number of parameters: %d' % n_params)\n","\n","    return trainer\n","\n","\n","class Trainer(object):\n","    \"\"\"\n","    Class that controls the training process.\n","\n","    Args:\n","            model(:py:class:`onmt.models.model.NMTModel`): translation model\n","                to train\n","            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n","               training loss computation\n","            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n","               training loss computation\n","            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n","               the optimizer responsible for update\n","            trunc_size(int): length of truncated back propagation through time\n","            shard_size(int): compute loss in shards of this size for efficiency\n","            data_type(string): type of the source input: [text|img|audio]\n","            norm_method(string): normalization methods: [sents|tokens]\n","            grad_accum_count(int): accumulate gradients this many times.\n","            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n","                the object that creates reports, or None\n","            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n","                used to save a checkpoint.\n","                Thus nothing will be saved if this parameter is None\n","    \"\"\"\n","\n","    def __init__(self,  args, model,  optim,\n","                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n","                  report_manager=None):\n","        # Basic attributes.\n","        self.args = args\n","        self.save_checkpoint_steps = args.save_checkpoint_steps\n","        self.model = model\n","        self.optim = optim\n","        self.grad_accum_count = grad_accum_count\n","        self.n_gpu = n_gpu\n","        self.gpu_rank = gpu_rank\n","        self.report_manager = report_manager\n","\n","        self.loss = torch.nn.BCELoss(reduction='none')\n","        assert grad_accum_count > 0\n","        # Set model in training mode.\n","        if (model):\n","            self.model.train()\n","\n","    def train(self, train_iter_fct, train_steps, valid_iter_fct=None, valid_steps=-1):\n","        \"\"\"\n","        The main training loops.\n","        by iterating over training data (i.e. `train_iter_fct`)\n","        and running validation (i.e. iterating over `valid_iter_fct`\n","\n","        Args:\n","            train_iter_fct(function): a function that returns the train\n","                iterator. e.g. something like\n","                train_iter_fct = lambda: generator(*args, **kwargs)\n","            valid_iter_fct(function): same as train_iter_fct, for valid data\n","            train_steps(int):\n","            valid_steps(int):\n","            save_checkpoint_steps(int):\n","\n","        Return:\n","            None\n","        \"\"\"\n","        logger.info('Start training...')\n","\n","        # step =  self.optim._step + 1\n","        step =  self.optim._step + 1\n","        true_batchs = []\n","        accum = 0\n","        normalization = 0\n","        train_iter = train_iter_fct()\n","\n","        total_stats = Statistics()\n","        report_stats = Statistics()\n","        self._start_report_manager(start_time=total_stats.start_time)\n","\n","        while step <= train_steps:\n","\n","            reduce_counter = 0\n","            for i, batch in enumerate(train_iter):\n","                if self.n_gpu == 0 or (i % self.n_gpu == self.gpu_rank):\n","\n","                    true_batchs.append(batch)\n","                    normalization += batch.batch_size\n","                    accum += 1\n","                    if accum == self.grad_accum_count:\n","                        reduce_counter += 1\n","                        if self.n_gpu > 1:\n","                            normalization = sum(\n","                                                all_gather_list\n","                                                (normalization))\n","\n","                        self._gradient_accumulation(\n","                            true_batchs, normalization, total_stats,\n","                            report_stats)\n","\n","                        report_stats = self._maybe_report_training(\n","                            step, train_steps,\n","                            self.optim.learning_rate,\n","                            report_stats)\n","\n","                        true_batchs = []\n","                        accum = 0\n","                        normalization = 0\n","                        if (step % self.save_checkpoint_steps == 0 and self.gpu_rank == 0):\n","                            self._save(step)\n","\n","                        step += 1\n","                        if step > train_steps:\n","                            break\n","            train_iter = train_iter_fct()\n","\n","        return total_stats\n","\n","    def validate(self, valid_iter, step=0):\n","        \"\"\" Validate model.\n","            valid_iter: validate data iterator\n","        Returns:\n","            :obj:`nmt.Statistics`: validation loss statistics\n","        \"\"\"\n","        # Set model in validating mode.\n","        self.model.eval()\n","        stats = Statistics()\n","\n","        with torch.no_grad():\n","            for batch in valid_iter:\n","\n","                src = batch.src\n","                labels = batch.labels\n","                segs = batch.segs\n","                clss = batch.clss\n","                mask = batch.mask\n","                mask_cls = batch.mask_cls\n","\n","                sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n","\n","\n","                loss = self.loss(sent_scores, labels.float())\n","                loss = (loss * mask.float()).sum()\n","                batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n","                stats.update(batch_stats)\n","            self._report_step(0, step, valid_stats=stats)\n","            return stats\n","\n","    def test(self, test_iter, step, cal_lead=False, cal_oracle=False):\n","        \"\"\" Validate model.\n","            valid_iter: validate data iterator\n","        Returns:\n","            :obj:`nmt.Statistics`: validation loss statistics\n","        \"\"\"\n","        # Set model in validating mode.\n","        def _get_ngrams(n, text):\n","            ngram_set = set()\n","            text_length = len(text)\n","            max_index_ngram_start = text_length - n\n","            for i in range(max_index_ngram_start + 1):\n","                ngram_set.add(tuple(text[i:i + n]))\n","            return ngram_set\n","\n","        def _block_tri(c, p):\n","            tri_c = _get_ngrams(3, c.split())\n","            for s in p:\n","                tri_s = _get_ngrams(3, s.split())\n","                if len(tri_c.intersection(tri_s))>0:\n","                    return True\n","            return False\n","\n","        if (not cal_lead and not cal_oracle):\n","            self.model.eval()\n","        stats = Statistics()\n","\n","        can_path = '%s_step%d.candidate'%(self.args.result_path,step)\n","        gold_path = '%s_step%d.gold' % (self.args.result_path, step)\n","        with open(can_path, 'w') as save_pred:\n","            with open(gold_path, 'w') as save_gold:\n","                with torch.no_grad():\n","                    for batch in test_iter:\n","                        src = batch.src\n","                        labels = batch.labels\n","                        segs = batch.segs\n","                        clss = batch.clss\n","                        mask = batch.mask\n","                        mask_cls = batch.mask_cls\n","\n","\n","                        gold = []\n","                        pred = []\n","\n","                        if (cal_lead):\n","                            selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n","                        elif (cal_oracle):\n","                            selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n","                                            range(batch.batch_size)]\n","                        else:\n","                            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n","\n","                            loss = self.loss(sent_scores, labels.float())\n","                            loss = (loss * mask.float()).sum()\n","                            batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n","                            stats.update(batch_stats)\n","\n","                            sent_scores = sent_scores + mask.float()\n","                            sent_scores = sent_scores.cpu().data.numpy()\n","                            selected_ids = np.argsort(-sent_scores, 1)\n","                        # selected_ids = np.sort(selected_ids,1)\n","                        for i, idx in enumerate(selected_ids):\n","                            _pred = []\n","                            if(len(batch.src_str[i])==0):\n","                                continue\n","                            for j in selected_ids[i][:len(batch.src_str[i])]:\n","                                if(j>=len( batch.src_str[i])):\n","                                    continue\n","                                candidate = batch.src_str[i][j].strip()\n","                                if(self.args.block_trigram):\n","                                    if(not _block_tri(candidate,_pred)):\n","                                        _pred.append(candidate)\n","                                else:\n","                                    _pred.append(candidate)\n","\n","                                if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == 3):\n","                                    break\n","\n","                            _pred = '<q>'.join(_pred)\n","                            if(self.args.recall_eval):\n","                                _pred = ' '.join(_pred.split()[:len(batch.tgt_str[i].split())])\n","\n","                            pred.append(_pred)\n","                            gold.append(batch.tgt_str[i])\n","\n","                        for i in range(len(gold)):\n","                            save_gold.write(gold[i].strip()+'\\n')\n","                        for i in range(len(pred)):\n","                            save_pred.write(pred[i].strip()+'\\n')\n","        if(step!=-1 and self.args.report_rouge):\n","            rouges = test_rouge(self.args.temp_dir, can_path, gold_path)\n","            logger.info('Rouges at step %d \\n%s' % (step, rouge_results_to_str(rouges)))\n","        self._report_step(0, step, valid_stats=stats)\n","\n","        return stats\n","\n","\n","\n","    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n","                               report_stats):\n","        if self.grad_accum_count > 1:\n","            self.model.zero_grad()\n","\n","        for batch in true_batchs:\n","            if self.grad_accum_count == 1:\n","                self.model.zero_grad()\n","\n","            src = batch.src\n","            labels = batch.labels\n","            segs = batch.segs\n","            clss = batch.clss\n","            mask = batch.mask\n","            mask_cls = batch.mask_cls\n","\n","            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n","\n","            loss = self.loss(sent_scores, labels.float())\n","            loss = (loss*mask.float()).sum()\n","            (loss/loss.numel()).backward()\n","            # loss.div(float(normalization)).backward()\n","\n","            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n","\n","\n","            total_stats.update(batch_stats)\n","            report_stats.update(batch_stats)\n","\n","            # 4. Update the parameters and statistics.\n","            if self.grad_accum_count == 1:\n","                # Multi GPU gradient gather\n","                if self.n_gpu > 1:\n","                    grads = [p.grad.data for p in self.model.parameters()\n","                             if p.requires_grad\n","                             and p.grad is not None]\n","                    all_reduce_and_rescale_tensors(\n","                        grads, float(1))\n","                self.optim.step()\n","\n","        # in case of multi step gradient accumulation,\n","        # update only after accum batches\n","        if self.grad_accum_count > 1:\n","            if self.n_gpu > 1:\n","                grads = [p.grad.data for p in self.model.parameters()\n","                         if p.requires_grad\n","                         and p.grad is not None]\n","                all_reduce_and_rescale_tensors(\n","                    grads, float(1))\n","            self.optim.step()\n","\n","    def _save(self, step):\n","        real_model = self.model\n","        # real_generator = (self.generator.module\n","        #                   if isinstance(self.generator, torch.nn.DataParallel)\n","        #                   else self.generator)\n","\n","        model_state_dict = real_model.state_dict()\n","        # generator_state_dict = real_generator.state_dict()\n","        checkpoint = {\n","            'model': model_state_dict,\n","            # 'generator': generator_state_dict,\n","            #'opt': self.args,\n","            'optim': self.optim,\n","        }\n","        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n","        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n","        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n","        if (not os.path.exists(checkpoint_path)):\n","            torch.save(checkpoint, checkpoint_path)\n","            return checkpoint, checkpoint_path\n","\n","    def _start_report_manager(self, start_time=None):\n","        \"\"\"\n","        Simple function to start report manager (if any)\n","        \"\"\"\n","        if self.report_manager is not None:\n","            if start_time is None:\n","                self.report_manager.start()\n","            else:\n","                self.report_manager.start_time = start_time\n","\n","    def _maybe_gather_stats(self, stat):\n","        \"\"\"\n","        Gather statistics in multi-processes cases\n","\n","        Args:\n","            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n","                or None (it returns None in this case)\n","\n","        Returns:\n","            stat: the updated (or unchanged) stat object\n","        \"\"\"\n","        if stat is not None and self.n_gpu > 1:\n","            return Statistics.all_gather_stats(stat)\n","        return stat\n","\n","    def _maybe_report_training(self, step, num_steps, learning_rate,\n","                               report_stats):\n","        \"\"\"\n","        Simple function to report training stats (if report_manager is set)\n","        see `onmt.utils.ReportManagerBase.report_training` for doc\n","        \"\"\"\n","        if self.report_manager is not None:\n","            return self.report_manager.report_training(\n","                step, num_steps, learning_rate, report_stats,\n","                multigpu=self.n_gpu > 1)\n","\n","    def _report_step(self, learning_rate, step, train_stats=None,\n","                     valid_stats=None):\n","        \"\"\"\n","        Simple function to report stats (if report_manager is set)\n","        see `onmt.utils.ReportManagerBase.report_step` for doc\n","        \"\"\"\n","        if self.report_manager is not None:\n","            return self.report_manager.report_step(\n","                learning_rate, step, train_stats=train_stats,\n","                valid_stats=valid_stats)\n","\n","    def _maybe_save(self, step):\n","        \"\"\"\n","        Save the model if a model saver is set\n","        \"\"\"\n","        if self.model_saver is not None:\n","            self.model_saver.maybe_save(step)\n","\n","print (\"Done\")"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TfI28hYCWkr7","colab_type":"text"},"source":["#### Train"]},{"cell_type":"code","metadata":{"id":"UcZVNwIKV4Xb","colab_type":"code","outputId":"1ecc2d62-8ada-40e7-da60-2e4f9ad1dc3d","executionInfo":{"status":"ok","timestamp":1572773118784,"user_tz":-120,"elapsed":17851,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from __future__ import division\n","\n","import argparse\n","import glob\n","import os\n","import random\n","import signal\n","import time\n","\n","import torch\n","from pytorch_pretrained_bert import BertConfig\n","\n","#import distributed\n","#from models import data_loader, model_builder\n","#from models.data_loader import load_dataset\n","#from models.model_builder import Summarizer\n","#from models.trainer import build_trainer\n","#from others.logging import logger, init_logger\n","\n","class Args(object):\n","    def __init__(self, kuku):\n","        self.kuku = kuku\n","\n","model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n","\n","\n","def str2bool(v):\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')\n","\n","\n","\n","def multi_main(args):\n","    \"\"\" Spawns 1 process per GPU \"\"\"\n","    init_logger()\n","\n","    nb_gpu = args.world_size\n","    mp = torch.multiprocessing.get_context('spawn')\n","\n","    # Create a thread to listen for errors in the child processes.\n","    error_queue = mp.SimpleQueue()\n","    error_handler = ErrorHandler(error_queue)\n","\n","    # Train with multiprocessing.\n","    procs = []\n","    for i in range(nb_gpu):\n","        device_id = i\n","        procs.append(mp.Process(target=run, args=(args,\n","            device_id, error_queue,), daemon=True))\n","        procs[i].start()\n","        logger.info(\" Starting process pid: %d  \" % procs[i].pid)\n","        error_handler.add_child(procs[i].pid)\n","    for p in procs:\n","        p.join()\n","\n","\n","\n","def run(args, device_id, error_queue):\n","\n","    \"\"\" run process \"\"\"\n","    setattr(args, 'gpu_ranks', [int(i) for i in args.gpu_ranks])\n","\n","    try:\n","        gpu_rank = multi_init(device_id, args.world_size, args.gpu_ranks)\n","        print('gpu_rank %d' %gpu_rank)\n","        if gpu_rank != args.gpu_ranks[device_id]:\n","            raise AssertionError(\"An error occurred in \\\n","                  Distributed initialization\")\n","\n","        train(args,device_id)\n","    except KeyboardInterrupt:\n","        pass  # killed by parent, do nothing\n","    except Exception:\n","        # propagate exception to parent process, keeping original traceback\n","        import traceback\n","        error_queue.put((args.gpu_ranks[device_id], traceback.format_exc()))\n","\n","\n","class ErrorHandler(object):\n","    \"\"\"A class that listens for exceptions in children processes and propagates\n","    the tracebacks to the parent process.\"\"\"\n","\n","    def __init__(self, error_queue):\n","        \"\"\" init error handler \"\"\"\n","        import signal\n","        import threading\n","        self.error_queue = error_queue\n","        self.children_pids = []\n","        self.error_thread = threading.Thread(\n","            target=self.error_listener, daemon=True)\n","        self.error_thread.start()\n","        signal.signal(signal.SIGUSR1, self.signal_handler)\n","\n","    def add_child(self, pid):\n","        \"\"\" error handler \"\"\"\n","        self.children_pids.append(pid)\n","\n","    def error_listener(self):\n","        \"\"\" error listener \"\"\"\n","        (rank, original_trace) = self.error_queue.get()\n","        self.error_queue.put((rank, original_trace))\n","        os.kill(os.getpid(), signal.SIGUSR1)\n","\n","    def signal_handler(self, signalnum, stackframe):\n","        \"\"\" signal handler \"\"\"\n","        for pid in self.children_pids:\n","            os.kill(pid, signal.SIGINT)  # kill children processes\n","        (rank, original_trace) = self.error_queue.get()\n","        msg = \"\"\"\\n\\n-- Tracebacks above this line can probably\n","                 be ignored --\\n\\n\"\"\"\n","        msg += original_trace\n","        raise Exception(msg)\n","\n","\n","\n","def wait_and_validate(args, device_id):\n","\n","    timestep = 0\n","    if (args.test_all):\n","        cp_files = sorted(glob.glob(os.path.join(args.model_path, 'model_step_*.pt')))\n","        cp_files.sort(key=os.path.getmtime)\n","        xent_lst = []\n","        for i, cp in enumerate(cp_files):\n","            step = int(cp.split('.')[-2].split('_')[-1])\n","            xent = validate(args,  device_id, cp, step)\n","            xent_lst.append((xent, cp))\n","            max_step = xent_lst.index(min(xent_lst))\n","            if (i - max_step > 10):\n","                break\n","        xent_lst = sorted(xent_lst, key=lambda x: x[0])[:3]\n","        logger.info('PPL %s' % str(xent_lst))\n","        for xent, cp in xent_lst:\n","            step = int(cp.split('.')[-2].split('_')[-1])\n","            test(args,  device_id, cp, step)\n","    else:\n","        while (True):\n","            cp_files = sorted(glob.glob(os.path.join(args.model_path, 'model_step_*.pt')))\n","            cp_files.sort(key=os.path.getmtime)\n","            if (cp_files):\n","                cp = cp_files[-1]\n","                time_of_cp = os.path.getmtime(cp)\n","                if (not os.path.getsize(cp) > 0):\n","                    time.sleep(60)\n","                    continue\n","                if (time_of_cp > timestep):\n","                    timestep = time_of_cp\n","                    step = int(cp.split('.')[-2].split('_')[-1])\n","                    validate(args,  device_id, cp, step)\n","                    test(args,  device_id, cp, step)\n","\n","            cp_files = sorted(glob.glob(os.path.join(args.model_path, 'model_step_*.pt')))\n","            cp_files.sort(key=os.path.getmtime)\n","            if (cp_files):\n","                cp = cp_files[-1]\n","                time_of_cp = os.path.getmtime(cp)\n","                if (time_of_cp > timestep):\n","                    continue\n","            else:\n","                time.sleep(300)\n","\n","\n","def validate(args,  device_id, pt, step):\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","    if (pt != ''):\n","        test_from = pt\n","    else:\n","        test_from = args.test_from\n","    logger.info('Loading checkpoint from %s' % test_from)\n","    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n","    opt = vars(checkpoint['opt'])\n","    for k in opt.keys():\n","        if (k in model_flags):\n","            setattr(args, k, opt[k])\n","    print(args)\n","\n","    config = BertConfig.from_json_file(args.bert_config_path)\n","    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n","    model.load_cp(checkpoint)\n","    model.eval()\n","\n","    valid_iter = Dataloader(args, load_dataset(args, 'valid', shuffle=False),\n","                                  args.batch_size, device,\n","                                  shuffle=False, is_test=False)\n","    trainer = build_trainer(args, device_id, model, None)\n","    stats = trainer.validate(valid_iter, step)\n","    return stats.xent()\n","\n","def test(args, device_id, pt, step):\n","\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","    if (pt != ''):\n","        test_from = pt\n","    else:\n","        test_from = args.test_from\n","    logger.info('Loading checkpoint from %s' % test_from)\n","    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n","    opt = vars(checkpoint['opt'])\n","    for k in opt.keys():\n","        if (k in model_flags):\n","            setattr(args, k, opt[k])\n","    print(args)\n","\n","    config = BertConfig.from_json_file(args.bert_config_path)\n","    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n","    model.load_cp(checkpoint)\n","    model.eval()\n","\n","    test_iter = Dataloader(args, load_dataset(args, 'test', shuffle=False),\n","                                  args.batch_size, device,\n","                                  shuffle=False, is_test=True)\n","    trainer = build_trainer(args, device_id, model, None)\n","    trainer.test(test_iter,step)\n","\n","\n","def baseline(args, cal_lead=False, cal_oracle=False):\n","\n","    test_iter = Dataloader(args, load_dataset(args, 'test', shuffle=False),\n","                                  args.batch_size, device,\n","                                  shuffle=False, is_test=True)\n","\n","    trainer = build_trainer(args, device_id, None, None)\n","    #\n","    if (cal_lead):\n","        trainer.test(test_iter, 0, cal_lead=True)\n","    elif (cal_oracle):\n","        trainer.test(test_iter, 0, cal_oracle=True)\n","\n","def save_dict(args, device_id):\n","    save_to = args.save_dict_to\n","    if (save_to == \"\"):\n","      print (\"Arg save_dict_to missed\")\n","      return\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","    pt = args.test_from\n","    if (pt != ''):\n","        test_from = pt\n","    else:\n","        test_from = args.test_from\n","    logger.info('Loading checkpoint from %s' % test_from)\n","    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n","    '''\n","    opt = vars(checkpoint['opt'])\n","    for k in opt.keys():\n","        if (k in model_flags):\n","            setattr(args, k, opt[k])\n","    print(args)\n","    '''\n","    config = BertConfig.from_json_file(args.bert_config_path)\n","    logger.info('Create model...')\n","    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n","    logger.info('Load checkpoint into model...')\n","    model.load_cp(checkpoint)\n","    logger.info(\"Save weights...\")\n","    torch.save(model.state_dict(), save_to)\n","\n","        \n","def train(args, device_id):\n","    init_logger(args.log_file)\n","\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","    logger.info('Device ID %d' % device_id)\n","    logger.info('Device %s' % device)\n","    torch.manual_seed(args.seed)\n","    random.seed(args.seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    if device_id >= 0:\n","        torch.cuda.set_device(device_id)\n","        torch.cuda.manual_seed(args.seed)\n","\n","\n","    torch.manual_seed(args.seed)\n","    random.seed(args.seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","    def train_iter_fct():\n","        return Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n","                                                 shuffle=True, is_test=False)\n","\n","    model = Summarizer(args, device, load_pretrained_bert=True)\n","    if args.train_from != '':\n","        logger.info('Loading checkpoint from %s' % args.train_from)\n","        checkpoint = torch.load(args.train_from,\n","                                map_location=lambda storage, loc: storage)\n","        #checkpoint = model.load_state_dict(torch.load(args.train_from))\n","        '''\n","        opt = vars(checkpoint['opt'])\n","        for k in opt.keys():\n","            if (k in model_flags):\n","                setattr(args, k, opt[k])\n","        '''\n","        model.load_cp(checkpoint)\n","        optim = build_optim(args, model, checkpoint)\n","    else:\n","        optim = build_optim(args, model, None)\n","\n","    logger.info(model)\n","    trainer = build_trainer(args, device_id, model, optim)\n","    trainer.train(train_iter_fct, args.train_steps)\n","\n","\n","'''\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","\n","\n","\n","    parser.add_argument(\"-encoder\", default='classifier', type=str, choices=['classifier','transformer','rnn','baseline'])\n","    parser.add_argument(\"-mode\", default='train', type=str, choices=['train','validate','test'])\n","    parser.add_argument(\"-bert_data_path\", default='../bert_data/bert')\n","    parser.add_argument(\"-model_path\", default='../models/')\n","    parser.add_argument(\"-result_path\", default='../results/cnndm')\n","    parser.add_argument(\"-temp_dir\", default='../temp')\n","    parser.add_argument(\"-bert_config_path\", default='../bert_config_uncased_base.json')\n","\n","    parser.add_argument(\"-batch_size\", default=1000, type=int)\n","\n","    parser.add_argument(\"-use_interval\", type=str2bool, nargs='?',const=True,default=True)\n","    parser.add_argument(\"-hidden_size\", default=128, type=int)\n","    parser.add_argument(\"-ff_size\", default=512, type=int)\n","    parser.add_argument(\"-heads\", default=4, type=int)\n","    parser.add_argument(\"-inter_layers\", default=2, type=int)\n","    parser.add_argument(\"-rnn_size\", default=512, type=int)\n","\n","    parser.add_argument(\"-param_init\", default=0, type=float)\n","    parser.add_argument(\"-param_init_glorot\", type=str2bool, nargs='?',const=True,default=True)\n","    parser.add_argument(\"-dropout\", default=0.1, type=float)\n","    parser.add_argument(\"-optim\", default='adam', type=str)\n","    parser.add_argument(\"-lr\", default=1, type=float)\n","    parser.add_argument(\"-beta1\", default= 0.9, type=float)\n","    parser.add_argument(\"-beta2\", default=0.999, type=float)\n","    parser.add_argument(\"-decay_method\", default='', type=str)\n","    parser.add_argument(\"-warmup_steps\", default=8000, type=int)\n","    parser.add_argument(\"-max_grad_norm\", default=0, type=float)\n","\n","    parser.add_argument(\"-save_checkpoint_steps\", default=5, type=int)\n","    parser.add_argument(\"-accum_count\", default=1, type=int)\n","    parser.add_argument(\"-world_size\", default=1, type=int)\n","    parser.add_argument(\"-report_every\", default=1, type=int)\n","    parser.add_argument(\"-train_steps\", default=1000, type=int)\n","    parser.add_argument(\"-recall_eval\", type=str2bool, nargs='?',const=True,default=False)\n","\n","\n","    parser.add_argument('-visible_gpus', default='-1', type=str)\n","    parser.add_argument('-gpu_ranks', default='0', type=str)\n","    parser.add_argument('-log_file', default='../logs/cnndm.log')\n","    parser.add_argument('-dataset', default='')\n","    parser.add_argument('-seed', default=666, type=int)\n","\n","    parser.add_argument(\"-test_all\", type=str2bool, nargs='?',const=True,default=False)\n","    parser.add_argument(\"-test_from\", default='')\n","    parser.add_argument(\"-train_from\", default='')\n","    parser.add_argument(\"-report_rouge\", type=str2bool, nargs='?',const=True,default=True)\n","    parser.add_argument(\"-block_trigram\", type=str2bool, nargs='?', const=True, default=True)\n","\n","    args = parser.parse_args()\n","    args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n","\n","    init_logger(args.log_file)\n","    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","    device_id = 0 if device == \"cuda\" else -1\n","\n","    if(args.world_size>1):\n","        multi_main(args)\n","    elif (args.mode == 'train'):\n","        train(args, device_id)\n","    elif (args.mode == 'validate'):\n","        wait_and_validate(args, device_id)\n","    elif (args.mode == 'lead'):\n","        baseline(args, cal_lead=True)\n","    elif (args.mode == 'oracle'):\n","        baseline(args, cal_oracle=True)\n","    elif (args.mode == 'test'):\n","        cp = args.test_from\n","        try:\n","            step = int(cp.split('.')[-2].split('_')[-1])\n","        except:\n","            step = 0\n","        test(args, device_id, cp, step)\n","'''\n","print (\"Done\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s-OVtGjFaTkn","colab_type":"text"},"source":["#### Launcher"]},{"cell_type":"code","metadata":{"id":"puWY0vyZaXYl","colab_type":"code","outputId":"e7422cd8-a778-4fcf-f95a-2001ad9906cc","executionInfo":{"status":"ok","timestamp":1572773184571,"user_tz":-120,"elapsed":83630,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","rootPath = \"/content/gdrive/My Drive/smz/\"\n","\n","args = Args(\"buku\")\n","  \n","  #python train.py -mode train -encoder classifier -dropout 0.1 \n","  #      -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier \n","  #      -lr 2e-3 -visible_gpus 0,1,2  -gpu_ranks 0,1,2 -world_size 3 -report_every 50 \n","  #      -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 50000 \n","  #      -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000\n","\n","args.encoder = \"transformer\" # choices=['classifier','transformer','rnn','baseline']\n","args.mode = \"dict\"  # choices=['train','validate','test', 'dict']    \n","args.bert_data_path = rootPath + 'bert_data/bert.pt_data'\n","args.model_path = rootPath + \"models/\"\n","args.result_path = rootPath + \"results\"\n","args.temp_dir = rootPath + \"temp\"\n","args.bert_config_path = rootPath + 'bert_config_uncased_base.json'\n","args.batch_size = 3000\n","args.use_interval = True\n","args.hidden_size = 128\n","args.ff_size = 512\n","args.heads = 4\n","args.inter_layers = 2\n","args.rnn_size = 512\n","args.param_init = 0\n","args.param_init_glorot = True\n","args.dropout = 0.1\n","args.optim = \"adam\"\n","args.lr = 2e-3\n","args.beta1 = 0.9\n","args.beta2 = 0.999\n","args.decay_method = \"noam\"\n","args.warmup_steps = 10000\n","args.max_grad_norm = 0\n","args.save_checkpoint_steps = 1000\n","args.accum_count = 2\n","args.world_size = 1\n","args.report_every = 100\n","args.train_steps = 50000\n","args.recall_eval = False\n","args.visible_gpus = \"0\"\n","args.gpu_ranks = \"0\"\n","args.log_file = rootPath + \"log/bertTransformer\"\n","args.dataset = \"\"\n","args.seed = 666\n","args.test_all = False\n","args.test_from = rootPath + \"models/model_step_50000.pt\"\n","args.train_from = \"\"\n","args.save_dict_to = rootPath + \"models/model0311T.pt\"\n","args.report_rouge = False    \n","args.block_trigram = True\n","args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n","\n","init_logger(args.log_file)\n","device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n","device_id = 0 if device == \"cuda\" else -1\n","\n","if(args.world_size>1):\n","    multi_main(args)\n","elif (args.mode == 'train'):\n","    train(args, device_id)\n","elif (args.mode == 'validate'):\n","    wait_and_validate(args, device_id)\n","elif (args.mode == 'lead'):\n","    baseline(args, cal_lead=True)\n","elif (args.mode == 'oracle'):\n","    baseline(args, cal_oracle=True)\n","elif (args.mode == 'dict'):\n","  save_dict(args, device_id)\n","elif (args.mode == 'test'):\n","    cp = args.test_from\n","    try:\n","        step = int(cp.split('.')[-2].split('_')[-1])\n","    except:\n","        step = 0\n","    test(args, device_id, cp, step)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["[2019-11-03 09:25:50,247 INFO] Loading checkpoint from /content/gdrive/My Drive/smz/models/model_step_50000.pt\n","[2019-11-03 09:26:14,156 INFO] Create model...\n","[2019-11-03 09:26:22,201 INFO] Load checkpoint into model...\n","[2019-11-03 09:26:22,351 INFO] Save weights...\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QxIWVqEWs_Uk","colab_type":"code","outputId":"33aa0fd9-5147-485d-b655-da7a33719857","executionInfo":{"status":"ok","timestamp":1572773184577,"user_tz":-120,"elapsed":83628,"user":{"displayName":"Semion Ch","photoUrl":"","userId":"18438445915208811231"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print (\"It is all!\")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["It is all!\n"],"name":"stdout"}]}]}